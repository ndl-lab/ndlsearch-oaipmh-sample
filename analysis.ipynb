{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全国書誌データを分析してみよう！！\n",
    "国立国会図書館では全国書誌データと呼ばれる書誌データを作成し、誰でも利用可能な形で提供しています。\n",
    "全国書誌データは以下のような特色があります。\n",
    "\n",
    ">全国書誌データは、国立国会図書館が網羅的に収集した国内出版物の標準的な書誌情報です。  \n",
    ">書店で一般に購入できる書籍などの納入率は、95％以上です。  \n",
    ">官庁出版物や地方自治体出版物など一般に流通しにくいものも多く含みます。  \n",
    ">刊行された出版物が国立国会図書館に届いてから、おおむね4日後に新着書誌情報として提供し、1か月程度で完成した書誌情報を提供しています。   \n",
    "[全国書誌データ提供](https://www.ndl.go.jp/jp/data/data_service/jnb/index.html)\n",
    "\n",
    "全国書誌データの利用は**無償**で、かつ**申請等も不要**です。\n",
    "\n",
    "データの取得方法としては2019年6月現在、以下の4種類が提供されています。\n",
    "(※[全国書誌データ提供サービス一覧](https://www.ndl.go.jp/jp/data/data_service/jnb/faq.html)も参照のこと。)\n",
    "\n",
    "* 検索用API\n",
    "図書館システムの検索画面等から、国立国会図書館サーチの書誌データを検索し、その結果を取得・表示することができます。  \n",
    "[検索用API](https://www.ndl.go.jp/jp/data/data_service/jnb/ndl_search.html#iss01)  \n",
    "[APIのご利用について](https://iss.ndl.go.jp/information/api/)\n",
    "\n",
    "* ハーベスト用API\n",
    "国立国会図書館サーチからOAI-PMHにより書誌データを取得できます。全件収集等、大量のデータをまとめて取得することができます。  \n",
    "[ハーベスト用API](https://www.ndl.go.jp/jp/data/data_service/jnb/ndl_search.html#iss02)  \n",
    "[国立国会図書館サーチが提供するOAI-PMH](https://iss.ndl.go.jp/information/api/api-lists/oai-pmh_info/)  \n",
    "\n",
    "* RSS\n",
    "新着書誌情報、全国書誌及び全国書誌（電子書籍・電子雑誌編）を、国立国会図書館サーチの機能を用いてRSS形式（RSS2.0）で提供しています。  \n",
    "[国立国会図書館サーチが提供するRSS](https://iss.ndl.go.jp/information/api/api-lists/rss_info/#2)\n",
    "\n",
    "* TSVファイル\n",
    "全国書誌（電子書籍・電子雑誌編）をTSVファイル（タブ区切り形式のテキストファイル）で提供しています。  \n",
    "[全国書誌（電子書籍・電子雑誌編）TSVファイル一覧](https://www.ndl.go.jp/jp/data/data_service/jnb/ebej_tsv.html)\n",
    "\n",
    "<br>\n",
    "今回はこの全国書誌を利用して、<br>\n",
    "1. ハーベスト用APIを利用した全件取得  <br>\n",
    "2. 取得したデータの整形とクレンジング<br>\n",
    "3. 結果の可視化  <br>\n",
    "4. 応用編(グラフをアニメーションにする)  <br>\n",
    "\n",
    "を行っていきます。  \n",
    "基本的には上から順番にctrl+Enterを押していけば実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要なライブラリ群のインストール\n",
    "!pip install pycurl tqdm datetime pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ハーベスト用APIを利用した全件取得\n",
    "適当なハーベスタを用意して必要な断面を全件収集してみましょう。  \n",
    "pythonで簡単なハーベスタを書いておきましたので、自己責任でご利用ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ハーベスタ)rdf/xmlを扱うための前準備\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pycurl\n",
    "\n",
    "import time\n",
    "import codecs\n",
    "from io import StringIO,BytesIO\n",
    "\n",
    "#XMLの名前空間\n",
    "OAI='{http://www.openarchives.org/OAI/2.0/}'\n",
    "dc ='{http://purl.org/dc/elements/1.1/}'\n",
    "dcndl='{http://ndl.go.jp/dcndl/terms/}'\n",
    "dcterms='{http://purl.org/dc/terms/}'\n",
    "rdf='{http://www.w3.org/1999/02/22-rdf-syntax-ns#}'\n",
    "rdfs='{http://www.w3.org/2000/01/rdf-schema#}'\n",
    "foaf='{http://xmlns.com/foaf/0.1/}'\n",
    "\n",
    "#ElementTree(xmlを解析するライブラリ)にも名前空間を登録\n",
    "ET.register_namespace('',\"http://www.openarchives.org/OAI/2.0/\")\n",
    "ET.register_namespace('rdf', \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "ET.register_namespace('rdfs', \"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "ET.register_namespace('dc',\"http://purl.org/dc/elements/1.1/\")\n",
    "ET.register_namespace('dcterms',\"http://purl.org/dc/terms/\")\n",
    "ET.register_namespace('dcndl',\"http://ndl.go.jp/dcndl/terms/\")\n",
    "ET.register_namespace('xsi',\"http://www.w3.org/2001/XMLSchema-instance\")\n",
    "ET.register_namespace('schemaLocation',\"http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd\")\n",
    "ET.register_namespace('oai_dc',\"http://www.openarchives.org/OAI/2.0/oai_dc/\")\n",
    "ET.register_namespace('foaf',\"http://xmlns.com/foaf/0.1/\")\n",
    "ET.register_namespace('owl',\"http://www.w3.org/2002/07/owl#\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハーベスタ本体\n",
    "from datetime import datetime, date, timedelta\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pycurl\n",
    "\n",
    "import time\n",
    "import codecs\n",
    "from io import StringIO,BytesIO\n",
    "import urllib.request\n",
    "\n",
    "class OAI_harvester:\n",
    "    def __init__(self, outputxmlpath=\"xml_all.xml\", prefixname=\"dcndl\"):\n",
    "        self.outputxml = outputxmlpath\n",
    "        self.prefixname = prefixname\n",
    "        self.resumptiontoken = None\n",
    "        self.datasize = None\n",
    "        with open(self.outputxml, 'wb') as f:\n",
    "            print(\"initialize file\")\n",
    "\n",
    "    def _parse_xml_ndl(self):\n",
    "        tree = ET.parse('oaitmp.xml')\n",
    "        root = tree.getroot()\n",
    "        with codecs.open(self.outputxml, 'a', \"utf-8\") as f:\n",
    "            es_item = root.find(OAI + 'ListRecords').findall(OAI + 'record')\n",
    "            for item in es_item:  # OAI-PMHは「id <xml>」のようになっているので不要なid部分を消す\n",
    "                if item.find(OAI + 'metadata') is None:\n",
    "                    continue\n",
    "                item2 = item.find(OAI + 'metadata').find(rdf + 'RDF')\n",
    "                item_id = item.find(OAI + 'metadata').find(rdf + 'RDF').find(dcndl + \"BibAdminResource\").attrib[\n",
    "                    rdf + \"about\"].split(\"/\")[-1]\n",
    "                item_str = ET.tostring(item2, encoding='utf8', method='xml').decode()\n",
    "                item_str = item_str.replace(\"\\n\", \"\")\n",
    "                f.write(item_str + \"\\n\")\n",
    "        if root.find(OAI + 'ListRecords') is None:\n",
    "            self.resumptiontoken = None\n",
    "            return\n",
    "        token = root.find(OAI + 'ListRecords').find(OAI + \"resumptionToken\")\n",
    "        if token is None or token.text is None:\n",
    "            self.resumptiontoken = None\n",
    "            return\n",
    "        self.datasize = token.attrib[\"completeListSize\"]\n",
    "        self.resumptiontoken = token.text\n",
    "\n",
    "    def _download_xml(self, fromdate):\n",
    "        # b = io.BytesIO()\n",
    "        with open(\"oaitmp.xml\", 'wb') as f:\n",
    "            url = \"http://iss.ndl.go.jp/api/oaipmh?verb=ListRecords\"\n",
    "            if self.resumptiontoken is not None:\n",
    "                url += \"&resumptionToken=\" + self.resumptiontoken\n",
    "            else:\n",
    "                url += \"&from=\" + fromdate + \"&metadataPrefix=\" + self.prefixname + \"&set=\" + self.setname\n",
    "                print(url)\n",
    "            # print(url)\n",
    "            try:\n",
    "                data = urllib.request.urlopen(url)\n",
    "                f.write(data.read())\n",
    "                http_code = data.getcode()\n",
    "                if http_code == 200:\n",
    "                    retval = True\n",
    "                else:\n",
    "                    retval = False\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                retval = False\n",
    "        return retval\n",
    "\n",
    "    def getxml(self, setname, fromdate=None):\n",
    "        self.setname = setname\n",
    "        self.resumptiontoken = None\n",
    "        if fromdate is None:\n",
    "            # 最初の200件は条件を指定して取得する。fromとuntilで年度の期間を取得できるが、最大1年分\n",
    "            today = datetime.today()\n",
    "            fromdate = datetime.strftime(today - timedelta(days=364), '%Y-%m-%d')\n",
    "        self._download_xml(fromdate)\n",
    "        self._parse_xml_ndl()\n",
    "        print(self.datasize + \"件見つかりました。200件ずつ取得します\")\n",
    "        if self.resumptiontoken is not None:\n",
    "            self._parse_xml_ndl()\n",
    "            for index in tqdm(range(int(self.datasize) // 200 + 1)):\n",
    "                while not self._download_xml(fromdate):\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(1)\n",
    "                # print(\"downloading  file_count:\",index)\n",
    "                self._parse_xml_ndl()\n",
    "                if self.resumptiontoken is None:\n",
    "                    break\n",
    "        else:\n",
    "            print(\"エラーです。set名を確認してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば「小説・物語」(日本十進分類法で913)に分類される全国書誌(iss-ndl-opac-national)の書誌データは以下のようにして全件取得できます。  \n",
    "\n",
    "# **注意**：  「小説・物語」の場合、実行に2時間程度かかります。\n",
    "# 取得済の断面をhttp://lab.ndl.go.jp/dataset/xml_913.zip\n",
    "からダウンロードできるようにしてあります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai=OAI_harvester(outputxmlpath=\"xml_913.xml\")\n",
    "#実行時に下のコメントを外してください(誤操作防止)\n",
    "#oai.getxml(setname=\"iss-ndl-opac-national:913\",fromdate=\"2018-06-19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 取得したデータの整形とクレンジング\n",
    "1で取得したデータは1行に1書誌のxmlが収まった形式をしています。  \n",
    "まずは書誌がどんなデータ構造をしているのか覗いてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "with codecs.open(\"xml_novel.xml\", \"r\",\"utf-8\") as f:\n",
    "    xmlsample=f.readline()\n",
    "    xmlstr = minidom.parseString(xmlsample).toprettyxml(indent=\"   \")\n",
    "    print(xmlstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書誌データの中身を見ると、タイトルや著者などのほか、「出版社」や「出版年」といった情報がわかります。  \n",
    "今回は出版社名に注目して、  \n",
    "**「小説・物語の分野で多くの本を出版しているのはどの出版社なのか、また出版年代ごとに変化はあるのか」**  \n",
    "調べてみましょう。  \n",
    "  \n",
    "上で表示した書誌データを見る限り、  \n",
    "出版社は\n",
    "```\n",
    "<dcterms:publisher>\n",
    "         <foaf:Agent>      \n",
    "            <foaf:name>\n",
    "```\n",
    "を使うとよさそうです。  \n",
    "出版年は\n",
    "```\n",
    "<dcterms:date>\n",
    "```\n",
    "を使ってみましょう。\n",
    "  \n",
    "また、XMLのままではデータの取り回しが不便なので、抽出したデータはpandasのデータフレームで管理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#書誌データから出版社名と出版年だけ取り出してデータフレームに加工する\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with codecs.open(\"xml_novel.xml\", \"r\",\"utf-8\") as f:\n",
    "    xmlsample=f.readline()\n",
    "    publisherList=[]\n",
    "    dateList=[]\n",
    "    cnt=0\n",
    "    while xmlsample:\n",
    "        cnt+=1\n",
    "        #if cnt%10000==0:\n",
    "        #    print(cnt)\n",
    "        tree = ET.fromstring(xmlsample)\n",
    "        #print(tree)\n",
    "        #root = tree.getroot()\n",
    "        publisher=tree.find(dcndl+'BibResource').find(dcterms+'publisher')\n",
    "        publishername=publisher.find(foaf+'Agent').find(foaf+'name')\n",
    "        publishdate=tree.find(dcndl+'BibResource').find(dcterms+'date')\n",
    "        #cleandate=publishdate.text.replace(\".*([0-9\\.]+).*\",r\"\\1\",regex=True)\n",
    "        #print(publishername.text,cleandate)\n",
    "        #tmp_se = pd.Series( [ publishername.text, publishdate.text], index=analysis_df.columns )\n",
    "        #analysis_df = analysis_df.append( tmp_se, ignore_index=True )\n",
    "        publisherList.append(publishername.text)\n",
    "        dateList.append(publishdate.text)\n",
    "        xmlsample=f.readline()\n",
    "    analysis_df = pd.DataFrame({'publisher':publisherList,'date':dateList})                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このままでは出版年の中に「制作」や「19--」や\\[2011\\]のような変則的な表記が含まれてしまい、数値としての大小がわかりません。  \n",
    "出版年が西暦4桁の数値だけを抽出して持つようにデータをきれいにしましょう(このような処理を「クレンジング」と呼びます)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データのクレンジングをする\n",
    "cleandf=analysis_df.copy()\n",
    "#西暦4桁が含まれていれば抽出、含まれていなければ欠損値とする\n",
    "cleandf['date']=cleandf['date'].str.extract('([0-9]{4})')\n",
    "#欠損値を含む書誌を削除\n",
    "cleandf=cleandf.dropna(how='any')\n",
    "#残った書誌データの出版年を数値にする\n",
    "cleandf['date']=cleandf['date'].astype(\"int\")\n",
    "\n",
    "\n",
    "print(\"書誌データの出版年の分布\")\n",
    "print(cleandf.describe())\n",
    "print(\"\\nきれいになった書誌データ\")\n",
    "print(cleandf.head())\n",
    "#csvとして書き出す\n",
    "#cleandf.to_csv(\"clean_novel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 結果の可視化\n",
    "データをきれいにしたので、いよいよ可視化をしてみましょう。\n",
    "出版年を追った時の出版数の推移を折れ線グラフで表してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ここから始めたい人\n",
    "#cleandf=pd.read_csv(\"clean_novel.csv\")\n",
    "\n",
    "#出版年ごとに集計してグラフにしてみる\n",
    "df = pd.DataFrame(cleandf.groupby('date').count())\n",
    "df.columns=[\"count\"]\n",
    "print(df[\"count\"].sum())\n",
    "#多い順ベスト10\n",
    "print(df.nlargest(10, columns='count'))\n",
    "df.plot.line(title=u'小説・物語の出版数年次推移')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日本語文字化け対策\n",
    "plt.rcParams['font.family'] = 'Yu Mincho'\n",
    "#出版社ごとに集計して多い順に表にしてみる\n",
    "grp_df=cleandf.groupby('publisher').count()\n",
    "\n",
    "grp_df.columns=[\"count\"]\n",
    "\n",
    "print(grp_df.nlargest(20, columns='count'))\n",
    "\n",
    "grp_df.nlargest(20, columns='count').plot.bar(alpha=0.6, figsize=(15,8))\n",
    "plt.title(u'小説・物語の出版数ランキング', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定の出版年に絞り込んだランキングも出力可能です。\n",
    "1990年のランキングを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "year=2000\n",
    "#year年に出版された書籍を出版社ごとに集計して多い順に表にしてみる\n",
    "grp_df=cleandf[cleandf['date']==year].groupby('publisher').count()\n",
    "grp_df.columns=[\"count\"]\n",
    "#トップ20\n",
    "print(grp_df.nlargest(20, columns='count'))\n",
    "\n",
    "grp_df.nlargest(20, columns='count').plot.bar(alpha=0.6, figsize=(8,8))\n",
    "plt.title(u'小説・物語の出版数ランキング(%d)'% year, size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 応用編(グラフをアニメーションにする) \n",
    "最後に、グラフをアニメーションにしてみましょう。\n",
    "2000年以降、出版数でトップ10に入ったことのある出版社の出版数推移をアニメーションにしてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "publisherlist=[]\n",
    "for year in range(2000,2018):\n",
    "    grp_df=cleandf[cleandf['date']==year].groupby('publisher').count()\n",
    "    grp_df.columns=[\"count\"]\n",
    "    x=grp_df.nlargest(20, columns='count')\n",
    "    publisherlist.extend(list(x.index))\n",
    "\n",
    "#重複を取り除く\n",
    "publisherlist=list(set(publisherlist))\n",
    "print(publisherlist)\n",
    "\n",
    "#描画の準備\n",
    "fig,ax= plt.subplots(figsize=(12, 10))\n",
    "ims = []\n",
    "\n",
    "for year in range(2000,2019):\n",
    "    #ttl = plt.text(0.5, 1.01, year, horizontalalignment='center', verticalalignment='bottom', transform=ax.transAxes)\n",
    "    #txt = plt.text(year,year,year)\n",
    "    \n",
    "    grp_df=cleandf[cleandf[\"date\"]==year].groupby(\"publisher\").count().reset_index()\n",
    "    grp_df.columns=[\"publisher\",\"count\"]\n",
    "    grp_df=grp_df[grp_df[\"publisher\"].isin(publisherlist)]\n",
    "    #x=grp_df.nlargest(20, columns='count')\n",
    "    #print(x[\"count\"])\n",
    "    im = plt.barh(list(grp_df[\"publisher\"]),grp_df[\"count\"].values)\n",
    "    #ax.text(.8,.8, \"{}\".format(year), transform=ax.transAxes)\n",
    "    plt.title(u'小説・物語の出版数推移(2000年から2018年)', size=16)\n",
    "    ims.append(im)\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims,  blit=False,interval=500)\n",
    "ani.save('出版推移.gif',writer='pillow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "今回の例では、近年20年間の小説の出版数を追跡することで  \n",
    "\n",
    "### 合併によって著しく出版数が変化する角川系、学研系の出版社\n",
    "### 自費出版中心の出版社(碧天舎、新風舎)の倒産\n",
    "### ケータイ小説等、小説投稿サイトを有する出版社(スターツ出版、アルファポリス)の台頭\n",
    "\n",
    "といった示唆的な可視化ができました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
